{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SentimentAnalysisUsing_RNN_LSTM_GRU.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DuD_wwhR3X9l","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import nltk\n","import re\n","from tensorflow.keras import models,layers\n","from nltk.corpus import stopwords \n","from nltk import RegexpTokenizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQ_F54BUtrZJ","colab_type":"text"},"source":["(a). Preprocessing the data , encoding the data and pading."]},{"cell_type":"code","metadata":{"id":"WKVeJb98swT-","colab_type":"code","colab":{}},"source":["train_data = open('/content/gdrive/My Drive/DeepLearningProjects/SentimentAnalysisTrainData.txt').readlines()\n","test_data = open('/content/gdrive/My Drive/DeepLearningProjects/SentimentAnalysisTestData.txt').readlines()\n","stop_words = set(stopwords.words('english')) \n","\n","def clean_Data(data):\n","  clean_sentences=[]\n","  clean = re.compile('<.*?>')\n","  for i in range(len(data)):\n","      data[i] =  re.sub(clean, '', data[i])\n","  tokenizer  = RegexpTokenizer(r'\\w+')\n","  for text in data:\n","    tokens = tokenizer.tokenize(text.lower())\n","    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n","    clean_sentences.append(tokens)\n","  clean_data = []\n","  for text in clean_sentences:\n","    sent = \" \".join(text)\n","    clean_data.append(sent)\n","  return clean_data\n","\n","def encode(train_data,test_data):\n","  tokenizer = Tokenizer()\n","  tokenizer.fit_on_texts(train_data+test_data)\n","  train_encoded = tokenizer.texts_to_sequences(train_data)\n","  test_encoded = tokenizer.texts_to_sequences(test_data)\n","  padded_train = tf.keras.preprocessing.sequence.pad_sequences(train_encoded,padding='post',maxlen=200)\n","  padded_test = tf.keras.preprocessing.sequence.pad_sequences(test_encoded,padding='post',maxlen=200)\n","  vocab_size = len(tokenizer.word_index)+1 \n","  return (vocab_size,padded_train,padded_test)\n","\n","vocab_size,x_train,x_test = encode(clean_Data(train_data),clean_Data(test_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMJQ7sw9tuan","colab_type":"text"},"source":["(b). Creating the target vectors for training and test data."]},{"cell_type":"code","metadata":{"id":"2dZyN3_rttL6","colab_type":"code","colab":{}},"source":["y_train         = [0]*25000\n","y_train[:12500] = [1]*12500\n","y_test          = [0]*25000\n","y_test[:12500]  = [1]*12500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5htUIPNZtsrx","colab_type":"text"},"source":["(c). Creating a validation set which is 20% of training data"]},{"cell_type":"code","metadata":{"id":"NJPWKN7DtsR8","colab_type":"code","colab":{}},"source":["#This can done while fitting the data using the validation split argument."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLC6PA1Ktv8x","colab_type":"text"},"source":["(d). Training the following models:"]},{"cell_type":"markdown","metadata":{"id":"-uu9WD0etr8w","colab_type":"text"},"source":["(e). **MODEL1**"]},{"cell_type":"code","metadata":{"id":"Buu02-DJtwtY","colab_type":"code","colab":{}},"source":["cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(num_units=200,activation='tanh')\n","model1 = models.Sequential()\n","model1.add(layers.Embedding(vocab_size,128))\n","model1.add(layers.RNN(cell=cell))\n","model1.add(layers.Dense(1,activation='sigmoid'))\n","model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","model1.fit(x_train,y_train,epochs=10,batch_size=30,validation_split=0.2,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8eG_un4Yt3eP","colab_type":"text"},"source":["(f). **MODEL2**"]},{"cell_type":"code","metadata":{"id":"gQCHoZgct3IY","colab_type":"code","colab":{}},"source":["model2 = models.Sequential()\n","model2.add(layers.Embedding(vocab_size,128))\n","model2.add(layers.LSTM(units=200,activation='tanh'))\n","model2.add(layers.Dense(1,activation='sigmoid'))\n","model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","model2.fit(x_train,y_train,epochs=10,batch_size=30,validation_split=0.2,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WsLZrbgTt23g","colab_type":"text"},"source":["(g). **MODEL3**"]},{"cell_type":"code","metadata":{"id":"NwSxxtATt2k3","colab_type":"code","colab":{}},"source":["model3 = models.Sequential()\n","model3.add(layers.Embedding(vocab_size,128))\n","model3.add(layers.GRU(units=200,activation='relu'))\n","model3.add(layers.Dense(1,activation='sigmoid'))\n","model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","model3.fit(x_train,y_train,epochs=10,batch_size=30,validation_split=0.2,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDSlu6a1t2VI","colab_type":"text"},"source":["(h). Among the models 1,2,3 the model2 is best performing as it has low loss and higher accuracy compared to others"]},{"cell_type":"code","metadata":{"id":"xv2vN7dxt2F1","colab_type":"code","colab":{}},"source":["(loss1,accuracy1) = model1.evaluate(x_test,y_test)\n","print(\"model1_loss:\",loss1,\"model1_accuracy:\",accuracy1)\n","(loss2,accuracy2) = model2.evaluate(x_test,y_test)\n","print(\"model2_loss:\",loss2,\"model2_accuracy:\",accuracy2)\n","(loss3,accuracy3) = model3.evaluate(x_test,y_test)\n","print(\"model3_loss:\",loss3,\"model3_accuracy:\",accuracy3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RGUNd6Cct11R","colab_type":"text"},"source":["(i). **MODEL4**"]},{"cell_type":"code","metadata":{"id":"bzRz6RFzt1lo","colab_type":"code","colab":{}},"source":["model4 = models.Sequential()\n","model4.add(layers.Embedding(vocab_size,128))\n","model4.add(layers.LSTM(units=200,return_sequences=True,activation='tanh'))\n","model4.add(layers.LSTM(units=200,activation='tanh'))\n","model4.add(layers.Dense(1,activation='sigmoid'))\n","model4.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","model4.fit(x_train,y_train,epochs=5,batch_size=30,validation_split=0.2,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1SwS6U6tt1Sv","colab_type":"text"},"source":["(j). **MODEL5**"]},{"cell_type":"code","metadata":{"id":"kaX2kVbWt01G","colab_type":"code","colab":{}},"source":["model5 = models.Sequential()\n","model5.add(layers.Embedding(vocab_size,128))\n","model5.add(layers.LSTM(units=200,return_sequence=True,activation='tanh'))\n","model5.add(layers.LSTM(units=200,activation='tanh'))\n","model5.add(layers.Dense(1,activation='sigmoid'))\n","model5.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","model5.fit(x_train,y_train,epochs=5,batch_size=30,validation_split=0.2,shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BlAQZXVZFMdV"},"source":["(l). Ploting loss vs iteration & accuracy vs iteration curves for training data for all the models."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EvyA1gKo_1Qi","colab":{}},"source":["#model1\n","(fig,axs) = plt.subplots(nrows=1,ncols=2)\n","ax  = axs[0]\n","ax.plot(model1.history.history['acc'])\n","ax.set_title(\"accuracy vs iterations\")\n","ax = axs[1]\n","ax.plot(model1.history.history['loss'])\n","ax.set_title(\"loss vs iterations\")\n","fig.suptitle(\"Model1\")\n","fig.show()\n","#model2\n","(fig,axs) = plt.subplots(nrows=1,ncols=2)\n","ax  = axs[0]\n","ax.plot(model2.history.history['acc'])\n","ax.set_title(\"accuracy vs iterations\")\n","ax = axs[1]\n","ax.plot(model2.history.history['loss'])\n","ax.set_title(\"loss vs iterations\")\n","fig.suptitle(\"Model2\")\n","fig.show()\n","#model3\n","(fig,axs) = plt.subplots(nrows=1,ncols=2)\n","ax  = axs[0]\n","ax.plot(model3.history.history['acc'])\n","ax.set_title(\"accuracy vs iterations\")\n","ax = axs[1]\n","ax.plot(model3.history.history['loss'])\n","ax.set_title(\"loss vs iterations\")\n","fig.suptitle(\"Model3\")\n","fig.show()\n","#model4\n","(fig,axs) = plt.subplots(nrows=1,ncols=2)\n","ax  = axs[0]\n","ax.plot(model4.history.history['acc'])\n","ax.set_title(\"accuracy vs iterations\")\n","ax = axs[1]\n","ax.plot(model4.history.history['loss'])\n","ax.set_title(\"loss vs iterations\")\n","fig.suptitle(\"Model4\")\n","fig.show()\n","#model5\n","(fig,axs) = plt.subplots(nrows=1,ncols=2)\n","ax  = axs[0]\n","ax.plot(model5.history.history['acc'])\n","ax.set_title(\"accuracy vs iterations\")\n","ax = axs[1]\n","ax.plot(model5.history.history['loss'])\n","ax.set_title(\"loss vs iterations\")\n","fig.suptitle(\"Model5\")\n","fig.show()"],"execution_count":null,"outputs":[]}]}